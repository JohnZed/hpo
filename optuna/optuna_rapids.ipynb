{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import dask.array as da\n",
    "from cuml.linear_model import LogisticRegression\n",
    "from cuml.preprocessing.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "import pandas as pd\n",
    "import optuna\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import cuml\n",
    "from cuml.ensemble import RandomForestClassifier\n",
    "import sklearn\n",
    "from cuml.metrics import accuracy_score\n",
    "\n",
    "from multiprocessing import Manager\n",
    "import random\n",
    "import time\n",
    "\n",
    "from joblib import parallel_backend\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import contextmanager\n",
    "import time\n",
    "\n",
    "@contextmanager\n",
    "def timed(name):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    t1 = time.time()\n",
    "    print(\"..%-24s:  %8.4f\" % (name, t1 - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TRIALS = 10\n",
    "INPUT_FILE = \"/home/hyperopt/data/air_par.parquet\"\n",
    "n_gpu = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rapids/lib/python3.6/site-packages/distributed/node.py:155: UserWarning:\n",
      "\n",
      "Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 40011 instead\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:37581</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:40011/status' target='_blank'>http://127.0.0.1:40011/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>2</li>\n",
       "  <li><b>Cores: </b>2</li>\n",
       "  <li><b>Memory: </b>49.16 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:37581' processes=2 threads=2, memory=49.16 GB>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "from dask.distributed import Client, wait\n",
    "from dask_cuda import LocalCUDACluster\n",
    "\n",
    "cluster = LocalCUDACluster()\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cudf.read_parquet(INPUT_FILE)\n",
    "X, y = df.drop([\"ArrDelayBinary\"], axis=1), df[\"ArrDelayBinary\"].astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Objective:\n",
    "    def __init__(self, gpu_queue):\n",
    "        # Shared queue to manage GPU IDs.\n",
    "        self.gpu_queue = gpu_queue\n",
    "\n",
    "    def __call__(self, trial):\n",
    "        # Fetch GPU ID for this trial.\n",
    "        gpu_id = self.gpu_queue.get()\n",
    "\n",
    "        # Please write actual objective function here.\n",
    "        max_depth = trial.suggest_int(\"max_depth\", 5, 7)\n",
    "        n_estimators = trial.suggest_int(\"n_estimators\", 100, 500)\n",
    "\n",
    "        classifier = RandomForestClassifier(max_depth=max_depth,\n",
    "                             n_estimators=n_estimators)\n",
    "\n",
    "\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(X, y)\n",
    "\n",
    "        classifier.fit(X_train, y_train)\n",
    "        y_pred = classifier.predict(X_valid)\n",
    "        score = accuracy_score(y_valid, y_pred)\n",
    "        \n",
    "        # Return GPU ID to the queue.\n",
    "        self.gpu_queue.put(gpu_id)\n",
    "\n",
    "        # GPU ID is stored as an objective value.\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2020-06-18 18:39:23,648]\u001b[0m A new study created with name: no-name-4862b2d8-356e-4b01-824f-4928898332f1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..multi-gpu               :   84.6736\n"
     ]
    }
   ],
   "source": [
    "with timed(\"multi-gpu\"):\n",
    "    study = optuna.create_study(storage=\"sqlite:///example.db\", direction=\"maximize\")\n",
    "\n",
    "    with Manager() as manager:\n",
    "\n",
    "        # Initialize the queue by adding available GPU IDs.\n",
    "        gpu_queue = manager.Queue()\n",
    "        for i in range(n_gpu):\n",
    "            gpu_queue.put(i)\n",
    "        with parallel_backend(\"dask\", n_jobs=n_gpu):\n",
    "            study.optimize(Objective(gpu_queue), n_trials=N_TRIALS, n_jobs=n_gpu)\n",
    "\n",
    "    # Show results.\n",
    "    study.trials_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_max_depth, params_n_estimators = study.trials_dataframe()['params_max_depth'], study.trials_dataframe()['params_n_estimators']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..sequential-call         :   71.2088\n"
     ]
    }
   ],
   "source": [
    "with timed(\"sequential-call\"):\n",
    "    max_acc = -1\n",
    "    with parallel_backend(\"dask\", n_jobs=n_gpu):\n",
    "        for i in range(N_TRIALS):\n",
    "            classifier = RandomForestClassifier(max_depth=params_max_depth[i],\n",
    "                                               n_estimators=params_n_estimators[i])\n",
    "\n",
    "            X_train, X_valid, y_train, y_valid = train_test_split(X, y)\n",
    "\n",
    "            classifier.fit(X_train, y_train)\n",
    "            y_pred = classifier.predict(X_valid)\n",
    "            score = accuracy_score(y_valid, y_pred)\n",
    "            if score > max_acc:\n",
    "                max_acc = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials:  10\n",
      "Best trial:\n",
      "  Value:  0.8311780095100403\n",
      "  Params: \n",
      "    max_depth: 7\n",
      "    n_estimators: 263\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def mlflow_callback(study, trial):\n",
    "#     trial_value = trial.value if trial.value is not None else float(\"nan\")\n",
    "#     with mlflow.start_run(run_name=study.study_name):\n",
    "#         mlflow.log_params(trial.params)\n",
    "#         mlflow.log_metrics({\"accuracy\": trial_value})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with timed(\"mlflow-gpu\"):\n",
    "#     study = optuna.create_study(direction=\"maximize\")\n",
    "#     study.optimize(objective, n_trials=N_TRIALS, timeout=600, callbacks=[mlflow_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CPU with 750 estimators max does not finish running after hours.\n",
    "# def objective_cpu(trial):\n",
    "    \n",
    "#     max_depth = trial.suggest_int(\"max_depth\", 5, 15)\n",
    "#     n_estimators = trial.suggest_int(\"n_estimators\", 100, 750)\n",
    "\n",
    "#     classifier = sklearn.ensemble.RandomForestRegressor(max_depth=max_depth,\n",
    "#                                        n_estimators=n_estimators)\n",
    "\n",
    "#     X_train, X_valid, y_train, y_valid = sklearn.model_selection.train_test_split(X_, y_)\n",
    "    \n",
    "#     classifier.fit(X_train, y_train)\n",
    "#     y_pred = classifier.predict(X_valid)\n",
    "    \n",
    "#     score = accuracy_score(y_valid, y_pred)\n",
    "#     return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with timed(\"cpu-etl\"):\n",
    "#     df_pd = pd.read_parquet(INPUT_FILE)\n",
    "#     X_, y_ = df_pd.drop([\"ArrDelayBinary\"], axis=1), df_pd[\"ArrDelayBinary\"].astype('int32')\n",
    "    \n",
    "# with timed(\"cpu-hpo\"):\n",
    "#     study = optuna.create_study(direction=\"maximize\") # Equivalent to an experiment, a set of trials\n",
    "#     study.optimize(objective_cpu, n_trials=N_TRIALS, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
